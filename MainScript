################################################################
# MAIN SCRIPT - DNA metabarcoding processing pipeline
################################################################
# Paige Smallman, 2025

# you may need to use cutadapt before running this to trim adapters
# MAKE SURE TO ADJUST VARIABLES AND PATHS BELOW

################################################################
# (1) SET UP & PARAMETERS
################################################################

# Load necessary libraries
# ---- Robust package installer/loader ----

cran_packages <- c(
  "here", "ggplot2", "tibble", "dplyr", "usethis", "Rcpp", "devtools", "remotes", "ape", "phangorn", "rBLAST", "tidyverse"
)
bioc_packages <- c(
  "dada2", "phyloseq", "speedyseq", "decontam", "metagMisc", "DECIPHER", "lulu",
  "Biostrings", "BiocGenerics", "XVector", "GenomeInfoDb", "ShortRead", "RcppParallel"
)
github_packages <- list(
  metagMisc = "vmikk/metagMisc",
  lulu = "tobiasgf/lulu",
  speedyseq = "mikemc/speedyseq"
)

# Install CRAN packages
to_install_cran <- setdiff(cran_packages, rownames(installed.packages()))
if(length(to_install_cran)) install.packages(to_install_cran)

# Install Bioconductor packages
if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager")
to_install_bioc <- setdiff(bioc_packages, rownames(installed.packages()))
if(length(to_install_bioc)) BiocManager::install(to_install_bioc, ask = FALSE, update = FALSE)

# Install GitHub packages if not present
if (!requireNamespace("remotes", quietly = TRUE)) install.packages("remotes")
for(pkg in names(github_packages)) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    remotes::install_github(github_packages[[pkg]])
  }
}

# Load all packages
all_packages <- unique(c(cran_packages, bioc_packages, names(github_packages)))
invisible(lapply(all_packages, function(pkg) library(pkg, character.only = TRUE)))

# USER-DEFINED VARIABLES; change these for specific project
project_id <- "ROHR05"              # Project identifier (e.g. ROHR05)
primer <- "12S"                     # Primers used (e.g. 12S, COI, 16S)
tax_db <- "MIDORI2_UNIQ_NUC_GB264_srRNA_DADA2.fasta"  # Taxonomy database

# Core directories (modify if folder structure changes)
input_root <- here("inputs")
output_root <- here("outputs", project_id)
figures_root <- here("figures", project_id)

# Create directories if they don't exist
dir.create(output_root, showWarnings = FALSE)
dir.create(figures_root, showWarnings = FALSE)

# Input paths
metadata_path <- here(input_root, "metadata", "metadata.csv")
trimmed_path <- here(input_root, "trimmed", project_id, primer)

# Output paths
dada2_output <- here(output_root, paste0(project_id, "_dada2.rds"))
track_table_path <- here(output_root, paste0(project_id, "_read_changes.txt"))
nochim_output <- here(output_root, paste0(project_id, "_nochim.rds"))
nochim_csv <- here(output_root, paste0(project_id, "_nochim.csv"))
tax_table_path <- here(output_root, paste0(project_id, "_taxtable.rds"))
phyloseq_path <- here(output_root, paste0(project_id, "_phyloseq.rds"))

# Taxonomy database path
tax_db_path <- here("tax", tax_db)

# Load metadata
metadata <- read.csv(metadata_path, header=TRUE, sep=",", row.names = NULL)

################################################################
# (2) PIPELINE EXECUTION
################################################################

# (A) Initial Quality Assessment
source("scripts/Quality_Assessment.R")  # This script is a function to generate quality plots for sequence reads

# Execute quality plotting
quality_data <- generate_quality_plots(
  pathinput = trimmed_path,
  pathfigures = figures_root
)
# load and view quality plots
quality_data

# (B) User-Defined Parameters (Modify After Viewing Plots)

# TRUNCATION PARAMETER GUIDELINES
# 1. truncLen: Set to positions where median quality > Q30
#    - Check 00_quality_forward.png and 00_quality_reverse.png
#    - Example: Forward drops at 220bp, Reverse at 200bp → c(220,200)
# 2. maxEE: Maximum expected errors (typically 2-3 for each read)
# 3. truncQ: Truncate when quality scores drop below this value (usually 2)

trunc_params <- list(
  truncLen = c(220, 200),  # Set based on quality plots
  maxEE = c(2, 2),         # Maximum expected errors
  truncQ = 2               # Truncate at first quality score < 2
)

# (C) This script filters + trims reads, infers Amplicon Sequence Variants (ASVs)

# for COI, remove non-specific amplification and filter by length 310–316 bp
COI_seqtab <- TRUE   # Set to TRUE if using COI primers, FALSE for 12S or others

source("scripts/Infer_ASVs_DADA2.R")    

# run DADA2 processing with ncores parellel processing 
processing_results <- run_dada2_processing(
  fnFs = quality_data$fnFs,
   fnRs = quality_data$fnRs,
   pathinput = trimmed_path,
   pathoutput = dada2_output,
   pathoutput_tracktable = track_table_path,
   pathoutput_nochim_rds = nochim_output,
   pathoutput_nochim_csv = nochim_csv,
   pathfigures = figures_root,
   trunc_params = trunc_params,
   ncores = 8
)

# (D) Taxonomy Assignment
source("scripts/Assign_Taxonomy_DADA2.R")   # This script assigns taxonomy using Dada2 Midori database to create an otu table

tax_results <- assign_taxonomy(
  seqtab_nochim_rds = nochim_output,
  tax_db_path = tax_db_path,
  output_dir = output_root,
  project_id = project_id,
  ncores = 8,
  seed = 119
)

# (E) Phyloseq Object Creation
source("scripts/Phyloseq.R")          # This script creates a phyloseq object

phy_results <- create_phyloseq(
  otu_table_rds = nochim_output,
  tax_table_rds = tax_table_path,
  metadata_path = metadata_path,
  output_dir = output_root,
  project_id = project_id,
  sample_id_col = 2,
  fix_mismatches = TRUE
)

################################################################
# (3) MARKER SPECIFIC STEPS
################################################################

## 12S #########################################################

## Remove contaminants with Decontam ####

source("scripts/Decontam_12S.R")

decon_results <- run_12S_decontamination(
  physeq_path = phyloseq_path,
  output_dir = file.path(output_root, "decontam"),
  project_id = project_id,
  control_col = "sample_type",
  neg_controls = c("extraction control", "field control", "pcr control"),
  min_reads = 1000,
  prevalence_threshold = 0.1
)

# Access clean phyloseq object
ps_clean <- decon_results$phyloseq_clean

## Assign taxonomy with BLAST ####

# === HIGH PERFORMANCE COMPUTING CLUSTER STEPS ===
# 1. Prepare ASV sequences for BLAST
asv_fasta <- file.path(output_root, paste0(project_id, "_ASVs.fasta"))
writeFasta(refseq(ps_clean), asv_fasta)

# 2. Transfer these files to HPC (like Hydra):
# - ASV sequences: {asv_fasta}
# - MIDORI database: (download from http://www.reference-midori.info/)
#   Example: MIDORI2_UNIQ_NUC_GB254_srRNA_BLAST

# 3. Create HPC submission script (blast_job.sh):
cat("
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=16G
#SBATCH --time=4:00:00

module load blast/2.13.0

# Format database (run once per database)
makeblastdb -in MIDORI2_UNIQ_NUC_GB254_srRNA_BLAST.fasta -dbtype nucl -parse_seqids

# Run BLAST with top 3 hits
blastn -db MIDORI2_UNIQ_NUC_GB254_srRNA_BLAST \
       -query", project_id, "_ASVs.fasta \
       -evalue 0.01 \
       -word_size 11 \
       -culling_limit 100 \
       -outfmt '6 qseqid sallseqid evalue bitscore length nident pident qcovs' \
       -out", project_id, "_BLAST_top3.out \
       -num_threads 8 \
       -max_target_seqs 3

# Transfer results back to local machine:
# scp", project_id, "_BLAST_top3.out user@local:/path/to/project/blast_results/
", file = here("scripts", "blast_job.sh"))

# === R PROCESSING STEPS ===
# 4. After receiving BLAST results, process in R:
source("scripts/BLAST_12S.R")

## Curation LCA with Galaxy tool ####

# === LOCAL R PROCESSING ===
source("scripts/LCA_Processing.R")

# 1. Prepare BLAST results for LCA
blast_path <- here("blast_results", paste0(project_id, "_BLAST_top3.out"))
lca_input <- prepare_blast_for_lca(
  blast_file = blast_path,
  output_dir = output_root,
  project_id = project_id
)

# 2. Generate HPC submission script
hpc_script <- create_lca_hpc_script(
  lca_input = lca_prep$lca_input_path,
  output_dir = output_root,
  project_id = project_id,
  identity_threshold = 80,
  coverage_threshold = 80,
  taxid_threshold = 98,
  taxcov_threshold = 80
)

# === HIGH PERFORMANCE COMPUTING CLUSTER STEPS ===
message("\n=== HPC EXECUTION STEPS ===\n",
        "1. Transfer these files to HPC:\n",
        "   - LCA input: ", lca_prep$lca_input_path, "\n",
        "   - Script: ", hpc_script, "\n\n",
        "2. Submit job with:\n",
        "   sbatch ", hpc_script, "\n",
        "3. Transfer back: ", project_id, "_LCA.txt")

# === POST-HPC R PROCESSING ===

# 3. After receiving LCA results
tax_table <- import_lca_results(
  lca_file = here("blast_results", paste0(project_id, "_LCA.txt")),
  output_dir = output_root,
  project_id = project_id
)

# 4. Validate and integrate taxonomy
if(validate_taxonomy(tax_table, ps_clean)) {
  tax_table(ps_clean) <- tax_table
  saveRDS(ps_clean, here(output_root, paste0(project_id, "_final_phyloseq.rds")))
} else {
  stop("Taxonomy validation failed - check LCA results")
}

# Yay! data is stored as final phyloseq rds!
# You can now continue to use for analysis!


## CO1 #########################################################

## Remove contaminants with Decontam ####
  # COI requires additional steps to address nuclear mitochondrial pseudogenes

source("scripts/Decontam_COI.R")

coi_decon <- run_COI_decontamination(
  physeq_path = phyloseq_path,
  output_dir = file.path(output_root, "coi_decontam"),
  project_id = project_id,
  control_col = "sample_type",
  neg_controls = c("extraction control", "field control", "pcr control"),
  min_reads = 2000,  # Higher threshold for COI
  prevalence_threshold = 0.05,
  max_n_ratio = 0.001,  # Stricter for COI
  frame_shift_check = TRUE
)

ps_clean <- coi_decon$phyloseq_clean

## Cluster ASVs with Decipher ####

source("scripts/Cluster_ASVs_COI.R") # this script clusters similar ASVs into OTUs with 97% similarity

# Run clustering
ps_otu <- cluster_coi_asvs(
  ps_object = ps_clean,
  output_dir = here("results/COI_clustering"),
  project_id = "ROHR05",
  cutoff = 0.03,       # 97% similarity threshold
  min_coverage = 0.8,  # Minimum sequence overlap
)

## Create match list with Vsearch ####



## Curate OTUs with lulu ####

## Assign taxonomy with BLAST ####

## Curation LCA with Galaxy tool ####
