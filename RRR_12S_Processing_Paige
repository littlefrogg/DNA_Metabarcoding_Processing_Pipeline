################################################################
#####        RRR eDNA - 12S MiFish U - ROHR_Paige        #######
################################################################
#
# SCRIPT prepared by Matthieu Leray
# March 14th 2023
# Modified by Paige Smallman
# January 23 2025

# Steps followed

# (1) Assemble metadata
# (2) Filter raw reads & Infer Amplicon Sequence Variants (ASVs)
# (3) Assign taxonomy
# (4) Create Phyloseq object
# (5) Remove contaminants, control and outlier samples
# (6) ASV curation
# (7) Taxonomic assignments with BLAST
# (8) Make a phylogenetic tree
# (9) Create curated Phyloseq object

# Load packages
library(dada2)
library(ggplot2)
library(phyloseq)
library(decontam)
library(metagMisc)
library(devtools)
library(lulu)
library(stringr)
library(DECIPHER)
library(phangorn)
library(rBLAST)
library(tidyverse)

################################################################
# (1) Assemble metadata
################################################################

# Sample list
SampleList <- "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/SampleList.csv"
#reading the sample data sheet
SampleList <- read.csv(SampleList, header=TRUE, sep = ",", row.names = NULL)

# DNA extract database
DNAdatabase <- "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/DNAEXTRACTSdatabase.csv"
#reading the sample data sheet
DNAdatabase <- read.csv(DNAdatabase, header=TRUE, sep = ",", row.names = NULL)

# Combine the two tables
joined_df <- merge(SampleList, DNAdatabase, by = "ml_id",all.x = TRUE, all.y = FALSE)
write.csv(joined_df, '~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/joined.csv', row.names = FALSE)

# Open updated metadata
joined_upt <- "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/joined.csv"
#reading the sample data sheet
metadata <- read.csv(joined_upt, header=TRUE, sep = ",", row.names = NULL)


################################################################
# (2) Filter raw reads & Infer Amplicon Sequence Variants (ASVs)
################################################################

##Creating filepaths to data 
path <- "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/inputs/trimmed"
head(list.files(path)) #eventually to check if the path works

##File preparation
#extracting Forward (fnFs) and Reverse (fnRs) reads from files
fnFs <- sort(list.files(path, pattern = "_R1_001.trimmed.fastq"))
fnRs <- sort(list.files(path, pattern = "_R2_001.trimmed.fastq"))
sample.names <- sapply(strsplit(fnFs, "_"), `[`, 1)
fnFs <-file.path(path, fnFs)
fnRs <-file.path(path, fnRs)

#plotting quality profiles
#qprofile_fwd <- print(plotQualityProfile(fnFs, aggregate = TRUE) 
#                      + ggtitle("Forward"))
#qprofile_rev <- print(plotQualityProfile(fnRs, aggregate = TRUE) 
#                      + ggtitle("Reverse"))

#placing filtered files in a new filtered subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names


#filtering and trimming, here truncation at 220 (Fwd) and 180 (Rev) bp, 
#2expected errors max (N discarded automatically)
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,180),
                     maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)

head(out) #eventually to check how filtering and trimming worked

#learning error rates
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)
#plotting errors
#plotErrors(errF, nominalQ=TRUE)
#plotErrors(errR, nominalQ=TRUE)

##Dereplicating reads
sam.names <- sapply(strsplit(basename(filtFs), "_"), `[`, 1)
derepFs <- derepFastq(filtFs)
names(derepFs) <- sam.names
derepRs <- derepFastq(filtRs)
names(derepRs) <- sam.names

##Infering Sequence Variants
dadaFs <- dada(derepFs, err = errF, pool = "pseudo", multithread = TRUE)
dadaFs[[1]]
dadaRs <- dada(derepRs, err = errR, pool = "pseudo", multithread = TRUE)
dadaRs[[1]]

##Merging paired ends
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)
ROHR05 <- makeSequenceTable(mergers)
dim(ROHR05)
#[1]   4 704
table(nchar(getSequences(ROHR05)))

#exporting files to use in the next part of the workflow
saveRDS(ROHR05, "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/12Stest.rds")



#identifying and removing chimeras
ROHR05.nochim <- removeBimeraDenovo(ROHR05, 
                                    method="pooled", 
                                    multithread=TRUE)
dim(ROHR05.nochim)
#[1]  4 682

#tracking changes through each step 
getN <- function(x) sum(getUniques(x))
track <-    cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN),
                  sapply(mergers, getN), rowSums(ROHR05.nochim))
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR",
                     "merged", "nonchim")
rownames(track) <- sam.names
write.table(track, "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/12Stest_read_changes.txt", sep = "\t", quote = FALSE,
            col.names=NA)

saveRDS(ROHR05.nochim, "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/12Stest.nochim.rds")

################################################################
# (3) Assign taxonomy using DADA2
################################################################
#see MiFishTestscript2 for cluster code

ROHR05.nochim = readRDS("~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/12Stest.nochim.rds")

#assigning taxonomy MIDORI2 version GB254
#reference dataset formatted for DADA2 can be found here: http://www.reference-midori.info/

##IN COMPUTE CANADA CLUSTER
#set.seed(119)
#ROHR05.nochim.tax <- assignTaxonomy(ROHR05.nochim, 
#                                    "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/tax/MIDORI2_UNIQ_NUC_GB263_srRNA_DADA2.fasta",
#                                    multithread=TRUE)
#saveRDS(ROHR05.nochim.tax, "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/ROHR05.nochim.tax.rds")

##BACK IN R
# Inspect taxonomic assignments
ROHR05.nochim.tax = readRDS(file = './ROHR05.nochim.tax.rds')
taxa.print <- ROHR05.nochim.tax # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

DADA_tax <- tax_table(ROHR05.nochim.tax)
write.csv(DADA_tax, file="~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/ROHR05.nochim.tax.csv")

DADA_otu = otu_table(ROHR05.nochim, taxa_are_rows = FALSE)
write.csv(DADA_otu, file="~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/ROHR05.nochim.tax.otu.csv")

################################################################
# (4) Create Phyloseq object
################################################################

# Read RDS object
ROHR05.nochim.tax = readRDS("~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/ROHR05.nochim.tax.rds")
ROHR05.nochim = readRDS("~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/12Stest.nochim.rds")

dim(ROHR05.nochim.tax)
# [1] 682 6
dim(ROHR05.nochim)
# [1]  4 682

##Creating phyloseq object
##Opening and extracting sample data from a .csv file
#creating path to the .csv 
ROHR05_sam <- "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/joined.csv"
#reading the sample data sheet
ROHR05_metadata <- read.csv(ROHR05_sam, header=TRUE, sep = ",", row.names = 1)

##TROUBLESHOOTING
##need to clean up ROHR05_metadata row names to match DADA_otu row names
#sample_names(DADA_otu) <- gsub("-COI-BAD", "", sample_names(DADA_otu))
#sample_names(DADA_otu) <- gsub("-COI-SABA", "", sample_names(DADA_otu))
#check if they match
#all(sample_names(DADA_otu) %in% rownames(ROHR05_metadata))
#inspect if false
#sample_names(DADA_otu) %in% rownames(ROHR05_metadata)
#turn DADA_otu into dataframe to check
#otu_df <- as.data.frame(otu_table(DADA_otu))


##Creating a unique phyloseq object with sample-by-sequence feature table, 
#the sample metadata and the sequence taxonomies 
ROHR05_obj <- phyloseq(otu_table(DADA_otu, taxa_are_rows = FALSE), 
                       sample_data(ROHR05_metadata), tax_table(DADA_tax)) 
ROHR05_obj

#phyloseq-class experiment-level object
#otu_table()   OTU Table:         [ 682 taxa and 4 samples ]
#sample_data() Sample Data:       [ 4 samples by 58 sample variables ]
#tax_table()   Taxonomy Table:    [ 682 taxa by 6 taxonomic ranks ]

# Save Phyloseq object
saveRDS(ROHR05_obj, "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/ROHR05_obj.rds")


################################################################
# (5) Remove contaminants, control and outlier samples
################################################################

ROHR05_obj = readRDS("~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/ROHR05_obj.rds")

# Identify Contaminants - Prevalence ##### R package decontam
sample_data(ROHR05_obj)$is.neg <- sample_data(ROHR05_obj)$Sample.type == "Control"
contamdf.prev <- isContaminant(ROHR05_obj, method="prevalence", neg="is.neg")
contaminants <- table(contamdf.prev$contaminant)
write.csv(contamdf.prev, file='~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/ROHR05_contamdf.prev.csv')

# Make phyloseq object of presence-absence in negative controls and true samples
ROHR05_obj.pa <- transform_sample_counts(ROHR05_obj, function(abund) 1*(abund>0))
ROHR05_obj.pa.neg <- prune_samples(sample_data(ROHR05_obj.pa)$Sample.type == "Control", ROHR05_obj.pa)
ROHR05_obj.pa.pos <- prune_samples(sample_data(ROHR05_obj.pa)$Sample.type == "eDNA", ROHR05_obj.pa)
# Make data.frame of prevalence in positive and negative samples
df.pa <- data.frame(pa.pos=taxa_sums(ROHR05_obj.pa.pos), pa.neg=taxa_sums(ROHR05_obj.pa.neg),
                    contaminant=contamdf.prev$contaminant)
ggplot(data=df.pa, aes(x=pa.neg, y=pa.pos, color=contaminant)) + geom_point() +
  xlab("Prevalence (Negative Controls)") + ylab("Prevalence (True Samples)")

# Create a new phyloseq object without contaminants identified in contamdf.prev
badTaxa = c("AGCCGCGGTAATACAGAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGCGCGTAGGCGGTTTGTTAAGTCGGATGTGAAATCCCAGGGCTCAACCTTGGAATGGCACCCGATACTGGCAGGCTAGAGTACGGTAGAGGGGTGTGGAATTTCCTGTGTAGCGGTGAAATGCGTAGATATAGGAAGGAACATCAGTGGCGAAGGCGACACCCTGGACCGATACTGACGCTGAGGTGCGAAAGCGTGGGGAG",
            "CGCCGCGGTAATACGGAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGCGCGTAGGTGGTCTGTTAAGCGGAATGTGAAAGCCCCGGGCTCAACCTGGGAACTGCATTGCGAACTGGCAGACTAGAGTACAGTAGAGGGTAGTGGAATTTCCTGTGTAGCGGTGAAATGCGTAGAGATGGGAAGGAACATCAGTGGCGAAGGCGACTGCCTGGACTGATACTGACACTGAGGTGCGAAAGCGTGGGGAG",
            "TCGTGCCAGCCACCGCGGTTATACGAGAGGCCCAAATCGATGCTCCACCGGCGTAAAGTGTGATTAGAGAAAACGAAAACTAAGGCCAAATATTCCTTATGCTGTCATACGCTAATAGGACACAAGAAGAACAACTACGAAAGTGGCTTTACTACCCTTGAACTCACGACAGCCAGATCACAAACTGGGATTAGATACCCCACTATGAGATCGGAAGAGCACACGTCTGA")

goodTaxa <- setdiff(taxa_names(ROHR05_obj), badTaxa)
ROHR05_obj <- prune_taxa(goodTaxa, ROHR05_obj)


# Remove all control samples
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "PCRI-12S-JL22-cntrl1")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "PCR2-12S-JL22-cntrl1")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "PCRI-12S-JL22-cntrl2")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "PCR2-12S-JL22-cntrl2")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "PCRI-12S-JL22-cntrl3")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "PCR2-12S-JL22-cntrl3")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML2991")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML3634")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML3639")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML3652")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML3673")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML3882")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML3895")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML3896")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML3913")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML3992")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML4001")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML4167")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML4218")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "EXT-12S-JL22-ML3621")


# Remove samples with less than 1,000 reads
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "BAD-12S-JL22-ML3635")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "BAD-12S-JL22-ML3636")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "BAD-12S-JL22-ML3638")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "CONA-12S-JL22-ML3640")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "CONA-12S-JL22-ML3990")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "UVA-12S-JL22-ML3875")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "UVA-12S-JL22-ML3876")
ROHR05_obj = subset_samples(ROHR05_obj, sample_names(ROHR05_obj) != "UVA-12S-JL22-ML4214")
sample_sums(ROHR05_obj)

#removings ASVs that are not present in any sample (if any)
ROHR05_obj <- prune_taxa(taxa_sums(ROHR05_obj) > 0, ROHR05_obj)
ROHR05_obj

#phyloseq-class experiment-level object
#otu_table()   OTU Table:         [ 6252 taxa and 148 samples ]
#sample_data() Sample Data:       [ 148 samples by 24 sample variables ]
#tax_table()   Taxonomy Table:    [ 6252 taxa by 6 taxonomic ranks ]

# Look at distribution of library size
df <- as.data.frame(sample_data(ROHR05_obj)) # Put sample_data into a ggplot-friendly data.frame
df$LibrarySize <- sample_sums(ROHR05_obj)
df <- df[order(df$LibrarySize),]
df$Index <- seq(nrow(df))
ggplot(data=df, aes(x=Index, y=LibrarySize, color=Sample.type)) + geom_point()

# Save Phyloseq object
saveRDS(ROHR05_obj, "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/ROHR05_obj_clean.rds")

################################################################
# (6) ASV curation
################################################################

ROHR05_obj_clean = readRDS("~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/ROHR05_obj_clean.rds")

DADA_tax2  = tax_table(ROHR05_obj_clean)
write.csv(DADA_tax2, file="~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/ROHR05_obj_clean.tax.csv")

DADA_otu2  = otu_table(ROHR05_obj_clean, taxa_are_rows = FALSE)
write.csv(DADA_otu2, file="~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/ROHR05_obj_clean.tax.otu.csv")

# File preparation
# A - OTU table with samples as columns and OTUs as rows
DADA_otu2  = t(DADA_otu2)
DADA_otu2.df  = phyloseq_to_df(DADA_otu2, addtax = F, addtot = F, addmaxrank = F,
                                   sorting = "abundance") # with package metagMisc
DADA_otu2.df  <- data.frame(DADA_otu2.df , row.names = 1)

# B - Fasta file to prepare the match list
#from manually created .csv file
ASV<- row.names(DADA_otu2.df)
ASV<- data.frame(ASV)
ASV$Seq<-ASV$ASV
ASV$ASV<-rep(paste0("ASV", seq(1,length(ASV$ASV))))
write.csv(ASV, file = "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/ROHR05_obj_clean.tax.seqs.csv", row.names = FALSE)

ROHR05.nochim.tax.seqs = read.csv("~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/ROHR05_obj_clean.tax.seqs.csv")
fa = character(2 * nrow(ROHR05.nochim.tax.seqs))
fa[c(TRUE, FALSE)] = paste0(">", ROHR05.nochim.tax.seqs$ASV)
fa[c(FALSE, TRUE)] = as.character(ROHR05.nochim.tax.seqs$Seq)
writeLines(fa, "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/ROHR05.nochim.tax.fasta")

# C - Create match list with vsearch
#download vsearch and add to local directory "local_eDNA"
#IN TERMINAL:
#cd /Users/paigesmallman/Documents/local_eDNA/vsearch-2.29.3-macos-aarch64/bin
# copy paste file "ROHR05.nochim.tax.fasta" in directory
# ./vsearch --usearch_global ROHR05.nochim.tax.fasta --db ROHR05.nochim.tax.fasta --self --id .84 --iddef 1 --userout match_list.txt -userfields query+target+id --maxaccepts 0 --query_cov .9 --maxhits 10
# this just created a match_list.txt file 
# copy paste to R working directory

#BACK IN R: 
matchlist_name = read.table("~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/tax/match_list.txt")
names(matchlist_name)[names(matchlist_name) == "V1"] <- "OTUid"
names(matchlist_name)[names(matchlist_name) == "V2"] <- "hit"
names(matchlist_name)[names(matchlist_name) == "V3"] <- "match"
matchlist_name$OTUid <- as.character(matchlist_name$OTUid)
matchlist_name$hit <- as.character(matchlist_name$hit)


### Run ASV curation
curated_result <- lulu(DADA_otu2.df, matchlist_name)

# Curated ASV table
curated_table = curated_result$curated_table
write.csv(curated_table, file="~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/curated_table.csv")

# Prepare fasta file from curated table
# from manually created .csv file
curated_table.seqs = read.csv("~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/curated_table.csv")
colnames(ASV) = c("ASV","X")
ASV_curated_merge = merge(ASV, curated_table.seqs, by = "X")
fa = character(2 * nrow(ASV_curated_merge))
fa[c(TRUE, FALSE)] = paste0(">", ASV_curated_merge$ASV)
fa[c(FALSE, TRUE)] = as.character(ASV_curated_merge$X)
writeLines(fa, "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/curated_ASV.seqs.fasta")


###########################################################################
# (7) Taxonomic assignments of with BLAST
###########################################################################

# Download latest MIDORI 12S unique database from http://www.reference-midori.info/download.php#
# (MIDORI2_UNIQ_NUC_GB263_srRNA_BLAST.zip)
# Unzip files and place curated_ASV.seqs.fasta in folder, then move folder to directory for cluster access

#IN COMPUTE CANADA CLUSTER
# to makeblastdb:
# nano mifishtest2_blast.sh
    # #!/bin/bash
    #SBATCH --time=02:00:00
    #SBATCH --mem-per-cpu=4G
    #SBATCH --cpus-per-task=4
    #SBATCH --job-name="mifishtest2"
    #SBATCH --account=def-barrett
    # module load gcc blast+
    # Create the nucleotide database based on `MIDORI2_UNIQ_NUC_GB254_srRNA_BLAST.fa`.
    #makeblastdb -in MIDORI2_UNIQ_NUC_SP_GB263_srRNA_BLAST.fasta -title Midori_Ref -dbtype nucl -out MIDORI2_UNIQ_NUC_SP_GB263_srRNA_BLAST.fasta
# sbatch mifishtest2_blast_db.sh

# For top THREE hits
# blastn -db MIDORI2_UNIQ_NUC_SP_GB263_srRNA_BLAST.fasta -query curated_ASV.seqs.fasta -evalue 0.01 -word_size 11 -culling_limit 100 -outfmt "6 qseqid sallseqid evalue bitscore length nident pident qcovs" -out curated_OTU-BLAST_top3.out -num_threads 8 -max_target_seqs 3

# Note on culling_limit
# "If the query range of a hit is enveloped by that of at least this many higher-scoring hits, delete the hit"
# the lower (5 - and default) number can produce odd results when there are several species with similar high scores.

#BACK IN R
# move files back to path accessible to R

# Edit output of blastn search so that it works as input in "galaxy-tool-lca"
# Import blast output 
blastout <- "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/curated_OTU-BLAST_top3.out"
#reading the sample data sheet
Blast.out <- read.csv(blastout, header=FALSE, sep="\t", row.names = NULL)


Blast.out$V2 <- gsub("root_1;", "", Blast.out$V2)
Blast.out$V2 <- gsub(";", " / ", Blast.out$V2)
Blast.out <- subset (Blast.out, select = -V5)
Blast.out <- subset (Blast.out, select = -V6)
Blast.out[c('#Subject accession', 'V2b')] <- str_split_fixed(Blast.out$V2, '###', 2)
Blast.out[c('Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species')] <- str_split_fixed(Blast.out$V2b, ' / ', 7)
Blast.out <- subset (Blast.out, select = -Kingdom)
Blast.out <- subset (Blast.out, select = -Phylum)
Blast.out <- subset (Blast.out, select = -Class)
Blast.out <- subset (Blast.out, select = -Order)
Blast.out <- subset (Blast.out, select = -Family)
Blast.out <- subset (Blast.out, select = -Genus)
Blast.out$Species2 <- gsub("^.*\\_","",Blast.out$Species)
Blast.out <- subset (Blast.out, select = -V2)

colnames(Blast.out)[colnames(Blast.out) == "V1"] = "#Query ID"
colnames(Blast.out)[colnames(Blast.out) == "V2b"] = "#Taxonomy"
colnames(Blast.out)[colnames(Blast.out) == "V3"] = "#evalue"
colnames(Blast.out)[colnames(Blast.out) == "V4"] = "#bitscore"
colnames(Blast.out)[colnames(Blast.out) == "V7"] = "#Identity percentage"
colnames(Blast.out)[colnames(Blast.out) == "V8"] = "#Coverage"
colnames(Blast.out)[colnames(Blast.out) == "Species"] = "#Subject"
colnames(Blast.out)[colnames(Blast.out) == "Species2"] = "#Subject Taxonomy ID"
Blast.out$Source = "Midori GB254"
colnames(Blast.out)[colnames(Blast.out) == "Source"] = "#Source"

col_order <- c("#Query ID", "#Subject", "#Subject accession", "#Subject Taxonomy ID", "#Identity percentage", "#Coverage", "#evalue", "#bitscore", "#Source", "#Taxonomy")
Blast.out <- Blast.out[, col_order]

# Export as tab delimited text ignoring row names
write.table(Blast.out, file = "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/OTU-LCA_top3.txt", sep = "\t", row.names = FALSE, quote = FALSE) 




# Summarize Lowest Common Ancestor with galaxy-tool-lca (https://github.com/naturalis/galaxy-tool-lca)

# python lca.py -i OTU-LCA_top3.txt -o curated_OTU-BLAST_top3.LCA.txt -b 8 -id 80 -cov 80 -t best_hits_range -tid 98 -tcov 80 -flh unknown

# Curated taxonomy table
tax_dada <- "~/Library/CloudStorage/OneDrive-McGillUniversity/Rstudio/eDNA/outputs/curated_OTU-BLAST_top3.LCA.txt"
tax_dada_blast <- read.csv(tax_dada, header=TRUE, sep = "\t")
tax_dada_blast <- tax_dada_blast[!duplicated(tax_dada_blast$X.Query), ]
tax_dada_blast2 <- tax_dada_blast[,-1]
rownames(tax_dada_blast2) <- tax_dada_blast[,1]
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.lca.rank"] = "lca.rank"
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.lca.taxon"] = "lca.taxon"
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.kingdom"] = "Kingdom"
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.phylum"] = "Phylum"
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.class"] = "Class"
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.order"] = "Order"
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.family"] = "Family"
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.genus"] = "Genus"
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.species"] = "Species"
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.method"] = "method"
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.identity"] = "identity"
colnames(tax_dada_blast2)[colnames(tax_dada_blast2) == "X.coverage"] = "coverage"
write.csv(tax_dada_blast2, file="~/Dropbox/STRI-RRR/Data/ROHR_05_miseq/eDNA_12S_Helio/curated_OTU-BLAST_top3.LCA.csv")


################################################################
# (8) Make a phylogenetic tree
################################################################

# Transpose curated_table
curated_table = t(curated_table)

# Extract sequences and names
asv_sequences.2 <- getSequences(curated_table)
names(asv_sequences.2)<-asv_sequences.2

nproc <- 8 # set to number of cpus/processors to use for the clustering

# Align sequences using DECIPHER
alignment <- AlignSeqs(DNAStringSet(asv_sequences.2), anchor=NA)

# Build maximum likelihood tree with Phangorn
phang.align <- phyDat(as(alignment, "matrix"), type="DNA") # Change sequence alignment output into a phyDat structure
dm <- dist.ml(phang.align) # Create distance matrix
treeNJ <- NJ(dm) # Perform Neighbor joining
fit = pml(treeNJ, data=phang.align) # Internal maximum likelihood

fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                    rearrangement = "stochastic", control = pml.control(trace = 0))


# Function to root the tree
pick_new_outgroup <- function(tree.unrooted){
  require("magrittr")
  require("data.table")
  require("ape") # ape::Ntip
  # tablify parts of tree that we need.
  treeDT <- 
    cbind(
      data.table(tree.unrooted$edge),
      data.table(length = tree.unrooted$edge.length)
    )[1:Ntip(tree.unrooted)] %>% 
    cbind(data.table(id = tree.unrooted$tip.label))
  # Take the longest terminal branch as outgroup
  new.outgroup <- treeDT[which.max(length)]$id
  return(new.outgroup)
}

new.outgroup = pick_new_outgroup(fitGTR$tree)
rootedTree = ape::root(fitGTR$tree, outgroup=new.outgroup, resolve.root=TRUE)

# plot(rootedTree)


################################################################
# (9) Create curated Phyloseq object
################################################################

# read FINAL metadata
Final_data_sam <- "~/Dropbox/STRI-RRR/Data/ROHR_05_miseq/eDNA_12S_Helio/joined_upt_final.csv"
Final_data_metadata <- read.csv(Final_data_sam, header=TRUE, sep = ",", row.names = 1)

# Transpose curated_table
curated_table = t(curated_table)

BCS_32_obj_curated <- phyloseq(otu_table(curated_table, taxa_are_rows = FALSE), 
                               sample_data(Final_data_metadata), tax_table(tax_dada_blast2), phy_tree(fitGTR$tree)) 

