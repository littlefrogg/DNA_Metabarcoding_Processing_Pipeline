################################################################
# MAIN SCRIPT - DNA metabarcoding processing pipeline
################################################################
# Paige Smallman, 2025

#you may need to use cutadapt before running this to trim adapters

################################################################
# (1) SET UP & PARAMETERS
################################################################

# User-defined variables; change these for specific project
project_id <- "ROHR05"              # Project identifier (e.g. ROHR05)
primer <- "12S"                     # Primers used (e.g. 12S, COI, 16S)
tax_db <- "MIDORI2_UNIQ_NUC_GB264_srRNA_DADA2.fasta"  # Taxonomy database

# Core directories (modify if folder structure changes)
input_root <- here("inputs")
output_root <- here("outputs", project_id)
figures_root <- here("figures", project_id)

# Create directories if they don't exist
dir.create(output_root, showWarnings = FALSE)
dir.create(figures_root, showWarnings = FALSE)

# Load packages
required_packages <- c("dada2", "here", "ggplot2", "phyloseq", "decontam", 
                       "metagMisc", "devtools", "lulu", "DECIPHER", "rBLAST",
                       "tidyverse", "phangorn")
invisible(lapply(required_packages, require, character.only = TRUE))

# Input paths
metadata_path <- here(input_root, "metadata", "metadata.csv")
trimmed_path <- here(input_root, "trimmed", project_id, primer)

# Output paths
dada2_output <- here(output_root, paste0(project_id, "_dada2.rds"))
track_table_path <- here(output_root, paste0(project_id, "_read_changes.txt"))
nochim_output <- here(output_root, paste0(project_id, "_nochim.rds"))
tax_table_path <- here(output_root, paste0(project_id, "_taxtable.rds"))
phyloseq_path <- here(output_root, paste0(project_id, "_phyloseq.rds"))

# Taxonomy database path
tax_db_path <- here("tax", tax_db)

# Load metadata
metadata <- read.csv(metadata_path, header=TRUE, sep=",", row.names = NULL)

################################################################
# (2) PIPELINE EXECUTION
################################################################

# (A) Initial Quality Assessment
source("scripts/Quality_Assessment.R")  # This script is a function to generate quality plots for sequence reads

# Execute quality plotting
quality_data <- generate_quality_plots(
  pathinput = trimmed_path,
  pathfigures = figures_root
)

# view quality plots

# (B) User-Defined Parameters (Modify After Viewing Plots)

# TRUNCATION PARAMETER GUIDELINES
# 1. truncLen: Set to positions where median quality > Q30
#    - Check 00_quality_forward.png and 00_quality_reverse.png
#    - Example: Forward drops at 220bp, Reverse at 200bp â†’ c(220,200)
# 2. maxEE: Maximum expected errors (typically 2-3 for each read)
# 3. truncQ: Truncate when quality scores drop below this value (usually 2)

trunc_params <- list(
  truncLen = c(220, 200),  # Set based on quality plots
  maxEE = c(2, 2),         # Maximum expected errors
  truncQ = 2               # Truncate at first quality score < 2
)

# (C) Infer ASVs
source("scripts/ASV_Processing.R")    # This script filters + trims reads, infers Amplicon Sequence Variants (ASVs)

processing_results <- run_dada2_processing(
  fnFs = quality_data$fnFs,
   fnRs = quality_data$fnRs,
   pathinput = trimmed_path,
   pathoutput = dada2_output,
   pathoutput_tracktable = track_table_path,
   pathoutput_nochim_rds = nochim_output,
   pathfigures = figures_root,
   trunc_params = trunc_params,
   ncores = 8
)

# (D) Taxonomy Assignment
source("scripts/Assign_Taxonomy_DADA2.R")   # This script assigns taxonomy using Dada2 Midori database to create an otu table

tax_results <- assign_taxonomy(
  seqtab_nochim_rds = nochim_output,
  tax_db_path = tax_db_path,
  output_dir = output_root,
  project_id = project_id,
  ncores = 8,
  seed = 119
)

# (E) Phyloseq Object Creation
source("scripts/Phyloseq.R")          # This script creates a phyloseq object

phy_results <- create_phyloseq(
  otu_table_rds = nochim_output,
  tax_table_rds = tax_table_path,
  metadata_path = metadata_path,
  output_dir = output_root,
  project_id = project_id,
  sample_id_col = 2,
  fix_mismatches = TRUE
)

################################################################
# (3) MARKER SPECIFIC STEPS
################################################################

## 12S #########################################################

## Remove contaminants with Decontam ####

source("scripts/Decontam_12S.R")

decon_results <- run_12S_decontamination(
  physeq_path = phyloseq_path,
  output_dir = file.path(output_root, "decontam"),
  project_id = project_id,
  control_col = "sample_type",
  neg_controls = c("extraction control", "field control", "pcr control"),
  min_reads = 1000,
  prevalence_threshold = 0.1
)

# Access clean phyloseq object
ps_clean <- decon_results$phyloseq_clean

## Assign taxonomy with BLAST ####

# === HIGH PERFORMANCE COMPUTING CLUSTER STEPS ===
# 1. Prepare ASV sequences for BLAST
asv_fasta <- file.path(output_root, paste0(project_id, "_ASVs.fasta"))
writeFasta(refseq(ps_clean), asv_fasta)

# 2. Transfer these files to HPC (like Hydra):
# - ASV sequences: {asv_fasta}
# - MIDORI database: (download from http://www.reference-midori.info/)
#   Example: MIDORI2_UNIQ_NUC_GB254_srRNA_BLAST

# 3. Create HPC submission script (blast_job.sh):
cat("
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=16G
#SBATCH --time=4:00:00

module load blast/2.13.0

# Format database (run once per database)
makeblastdb -in MIDORI2_UNIQ_NUC_GB254_srRNA_BLAST.fasta -dbtype nucl -parse_seqids

# Run BLAST with top 3 hits
blastn -db MIDORI2_UNIQ_NUC_GB254_srRNA_BLAST \
       -query", project_id, "_ASVs.fasta \
       -evalue 0.01 \
       -word_size 11 \
       -culling_limit 100 \
       -outfmt '6 qseqid sallseqid evalue bitscore length nident pident qcovs' \
       -out", project_id, "_BLAST_top3.out \
       -num_threads 8 \
       -max_target_seqs 3

# Transfer results back to local machine:
# scp", project_id, "_BLAST_top3.out user@local:/path/to/project/blast_results/
", file = here("scripts", "blast_job.sh"))

# === R PROCESSING STEPS ===
# 4. After receiving BLAST results, process in R:
source("scripts/BLAST_12S.R")

## Curation LCA with Galaxy tool ####

# === LOCAL R PROCESSING ===
source("scripts/LCA_Processing.R")

# 1. Prepare BLAST results for LCA
blast_path <- here("blast_results", paste0(project_id, "_BLAST_top3.out"))
lca_input <- prepare_blast_for_lca(
  blast_file = blast_path,
  output_dir = output_root,
  project_id = project_id
)

# 2. Generate HPC submission script
hpc_script <- create_lca_hpc_script(
  lca_input = lca_prep$lca_input_path,
  output_dir = output_root,
  project_id = project_id,
  identity_threshold = 80,
  coverage_threshold = 80,
  taxid_threshold = 98,
  taxcov_threshold = 80
)

# === HIGH PERFORMANCE COMPUTING CLUSTER STEPS ===
message("\n=== HPC EXECUTION STEPS ===\n",
        "1. Transfer these files to HPC:\n",
        "   - LCA input: ", lca_prep$lca_input_path, "\n",
        "   - Script: ", hpc_script, "\n\n",
        "2. Submit job with:\n",
        "   sbatch ", hpc_script, "\n",
        "3. Transfer back: ", project_id, "_LCA.txt")

# === POST-HPC R PROCESSING ===

# 3. After receiving LCA results
tax_table <- import_lca_results(
  lca_file = here("blast_results", paste0(project_id, "_LCA.txt")),
  output_dir = output_root,
  project_id = project_id
)

# 4. Validate and integrate taxonomy
if(validate_taxonomy(tax_table, ps_clean)) {
  tax_table(ps_clean) <- tax_table
  saveRDS(ps_clean, here(output_root, paste0(project_id, "_final_phyloseq.rds")))
} else {
  stop("Taxonomy validation failed - check LCA results")
}

# Yay! data is stored as final phyloseq rds!
# You can now continue to use for analysis!

## CO1 #########################################################

## Remove contaminants with Decontam
# COI requires additional steps to address nuclear mitochondrial pseudogenes

source("scripts/Decontam_COI.R")

coi_decon <- run_COI_decontamination(
  physeq_path = phyloseq_path,
  output_dir = file.path(output_root, "coi_decontam"),
  project_id = project_id,
  control_col = "sample_type",
  neg_controls = c("extraction control", "field control", "pcr control"),
  min_reads = 2000,  # Higher threshold for COI
  prevalence_threshold = 0.05,
  max_n_ratio = 0.001,  # Stricter for COI
  frame_shift_check = TRUE
)

# Cluster ASVs with Decipher


# Create match list with Vsearch

# Curate OTUs with lulu

## Assign taxonomy with BLAST

#Curation LCA with Galaxy tool
