################################################################
# DECONTAM COI - DNA metabarcoding processing
################################################################
# Paige Smallman, 2025

# this script removes contaminants, control and outlier samples

# decontam package provides statistical methods to identify, visualize, and remove contamination
# this script uses the prevalence method - relies on controls + compares the prevalence of sequences in the samples vs. controls
# if prevalence of a sequence is higher in control = contaminant

run_COI_decontamination <- function(
  physeq_path,
  asv_seqs_path,
  output_dir,
  project_id,
  control_col = "sample_type",
  neg_controls = c("extraction control", "field control", "pcr control"),
  min_reads = 300,
  prevalence_threshold = 0.05,
  max_n_ratio = 0.001,       # Stricter N filter for protein-coding
  frame_shift_check = TRUE,  # Check for pseudogenes
  manual_remove_taxa = NULL
  ) {
  
  suppressPackageStartupMessages({
    require(phyloseq)
    require(decontam)
    require(ggplot2)
    require(Biostrings)
    require(tidyverse)
  })
  
# Create output directory
dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)
  
# DATA LOADING & VALIDATION

message("\nLoading phyloseq object...")
ps <- readRDS(physeq_path)


# Load ASV sequences and match to phyloseq object
asv_seqs <- readRDS(asv_seqs_path)
asv_seqs <- asv_seqs[taxa_names(ps)]
# Save original, unfiltered asv_seqs for contaminant export
asv_seqs_all <- asv_seqs

# Validate sequences exist
if (is.null(asv_seqs) || length(asv_seqs) != ntaxa(ps)) {
  stop("ASV sequences must be provided as a DNAStringSet and match taxa_names(ps)")
}

# PRE-FILTERING FOR COI CHARACTERISTICS

message("\nApplying COI-specific pre-filters...")
  
# 1. Length filtering (remove extreme lengths)
seq_lengths <- width(asv_seqs)
length_cutoff <- quantile(seq_lengths, c(0.01, 0.99))
keep_length <- seq_lengths >= length_cutoff[1] & seq_lengths <= length_cutoff[2]

# 2. N-content filtering
n_content <- letterFrequency(asv_seqs, "N", as.prob=TRUE)
keep_n <- n_content <= max_n_ratio

# 3. Frame shift detection (requires alignment - simplified example)
if(frame_shift_check) {
  # In practice: Align to reference COI and check indels
  # Here we use simple length modulo 3 check
  keep_frame <- width(asv_seqs) %% 3 == 0
} else {
  keep_frame <- rep(TRUE, ntaxa(ps))
}

# Apply all filters
# Create a logical vector for filtering
keep <- keep_length & keep_n & keep_frame
# Ensure it's a named logical vector and not a matrix
keep <- as.vector(keep)
names(keep) <- taxa_names(ps)
ps <- prune_taxa(keep, ps)
asv_seqs <- asv_seqs[keep]
  
# CONTAMINANT IDENTIFICATION (SIMILAR TO 12S BUT MORE STRICT)

message("\nIdentifying contaminants with COI-specific parameters...")
  
# Mark negative controls
sample_data(ps)$is_neg <- sample_data(ps)[[control_col]] %in% neg_controls
  
# Use combined prevalence/frequency method for COI
contam_df <- isContaminant(
  ps,
  method = "prevalence",
  neg = "is_neg",
  threshold = prevalence_threshold,  # More conservative threshold
  normalize = TRUE,
  batch = NULL
)
## Remove debug printouts
  
# NUMT-SPECIFIC FILTERING
  
message("\nApplying NUMT detection heuristics...")
  
# 1. Sequence composition analysis
gc_content <- letterFrequency(asv_seqs, "GC", as.prob=TRUE)[,1]
gc_outliers <- gc_content < quantile(gc_content, 0.01) | gc_content > quantile(gc_content, 0.99)

# 2. Stop codon check (requires translation)
aa <- safe_translate(asv_seqs, if.fuzzy.codon="solve")
has_stop <- vcountPattern("*", aa) > 0

# 3. Evolutionary distance filtering (placeholder)
# In practice: Align to reference database and compute distances

# Combine NUMT indicators
numt_candidates <- gc_outliers | has_stop
  
# Mark contaminants and NUMTs
contam_df$numt <- numt_candidates
contam_df$contaminant <- contam_df$contaminant | numt_candidates

# VISUALIZATION & DIAGNOSTICS

message("\nGenerating COI-specific diagnostic plots...")
  
# NUMT diagnostic plot
  numt_plot <- ggplot(data.frame(gc_content, has_stop), 
                      aes(x=gc_content, fill=has_stop)) +
    geom_histogram(bins=30) +
    labs(x="GC Content", y="Count", title="NUMT Detection") +
    theme_minimal()
  
ggsave(file.path(output_dir, "numt_detection.png"), numt_plot, width=8, height=6)
  
# DECONTAMINATION & FILTERING
 
message("\nRemoving contaminants and NUMTs...")
  
# Remove contaminants and NUMTs
keep_taxa <- taxa_names(ps)[!contam_df$contaminant]
if (length(keep_taxa) == 0) {
  stop("No taxa remain after contaminant/NUMT filtering. Check your thresholds or contaminant detection.")
}
ps_clean <- prune_taxa(keep_taxa, ps)
if (ntaxa(ps_clean) == 0) stop("No taxa remain after contaminant/NUMT filtering.")

# Recalculate lib_size after pruning
sample_data(ps_clean)$lib_size <- sample_sums(ps_clean)

# Get sample names to keep
keep_samples <- sample_names(ps_clean)[sample_data(ps_clean)$lib_size >= min_reads]
if (length(keep_samples) == 0) stop("No samples remain after library size filtering.")

# Subset each component
otu <- otu_table(ps_clean)[, keep_samples, drop=FALSE]
tax <- tax_table(ps_clean)
samp <- sample_data(ps_clean)[keep_samples, , drop=FALSE]

# Rebuild phyloseq object
ps_clean <- phyloseq(otu, tax, samp)
if (nsamples(ps_clean) == 0) {
  stop("No samples remain after library size filtering.")
}

# Prune taxa with taxa_sums > 0
ps_clean <- prune_taxa(taxa_sums(ps_clean) > 0, ps_clean)
if (ntaxa(ps_clean) == 0) stop("No taxa remain after final pruning.")

# Manual removal step if needed
if (!is.null(manual_remove_taxa)) {
  ps_clean <- prune_taxa(!taxa_names(ps_clean) %in% manual_remove_taxa, ps_clean)
  message(paste0("Manually removed ", length(manual_remove_taxa), " taxa."))
  if (ntaxa(ps_clean) == 0) stop("No taxa remain after manual removal.")
}
  
# OUTPUT GENERATION

output_path <- file.path(output_dir, paste0(project_id, "_COI_clean.rds"))
saveRDS(ps_clean, output_path)
  
# Save contaminant sequences with metadata
contam_data <- data.frame(
  sequence = as.character(asv_seqs_all[rownames(contam_df)]),
  reason = ifelse(contam_df$numt, "NUMT", "Contaminant"),
  p_value = contam_df$p
)
write.csv(contam_data, file.path(output_dir, "coi_contaminants.csv"))

return(list(
  phyloseq_clean = ps_clean,
  output_path = output_path,
  contaminants = contam_df,
  numt_stats = table(numt_candidates),
  filtered_sequences = ntaxa(ps) - ntaxa(ps_clean)
))
}

# Helper function to translate DNA
safe_translate <- function(dna, ...) {
  cat("safe_translate called on", length(dna), "sequences\n")
  translated_sequences <- list()
  for (i in seq_along(dna)) {
    sequence <- as.character(dna[i])
    translated_aa <- tryCatch(
      Biostrings::translate(DNAString(sequence), ...),
      error = function(e) AAString("")
    )
    translated_sequences[[i]] <- translated_aa
  }
  return(AAStringSet(translated_sequences))
}