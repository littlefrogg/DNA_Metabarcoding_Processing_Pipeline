################################################################
# MAIN SCRIPT - DNA metabarcoding processing pipeline
################################################################

# COI TEST MAIN
# Aug 8, 2025
# R version 4.5

################################################################
# (1) SET UP & PARAMETERS
################################################################

# Load necessary libraries
cran_packages <- c(
  "here", "ggplot2", "tibble", "dplyr", "usethis", "Rcpp", "devtools", "remotes", "ape", "phangorn", "tidyverse"
)
bioc_packages <- c(
  "dada2", "phyloseq", "speedyseq", "decontam", "metagMisc", "DECIPHER", "lulu", "rBLAST",
  "Biostrings", "BiocGenerics", "XVector", "GenomeInfoDb", "ShortRead", "RcppParallel" 
)
github_packages <- list(
  metagMisc = "vmikk/metagMisc",
  lulu = "tobiasgf/lulu",
  speedyseq = "mikemc/speedyseq"
)

# Install CRAN packages
to_install_cran <- setdiff(cran_packages, rownames(installed.packages()))
if(length(to_install_cran)) install.packages(to_install_cran)

# Install Bioconductor packages
if (!requireNamespace("BiocManager", quietly = TRUE)) install.packages("BiocManager")
to_install_bioc <- setdiff(bioc_packages, rownames(installed.packages()))
if(length(to_install_bioc)) BiocManager::install(to_install_bioc, ask = FALSE, update = FALSE)

# Install GitHub packages if not present
if (!requireNamespace("remotes", quietly = TRUE)) install.packages("remotes")
for(pkg in names(github_packages)) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    remotes::install_github(github_packages[[pkg]])
  }
}

# Load all packages
all_packages <- unique(c(cran_packages, bioc_packages, names(github_packages)))
invisible(lapply(all_packages, function(pkg) library(pkg, character.only = TRUE)))

# USER-DEFINED VARIABLES; change these for specific project
project_id <- "testCOI"              # Project identifier (e.g. ROHR05)
primer <- "COI"                     # Primers used (e.g. 12S, COI, 16S)
tax_db <- "MIDORI2_UNIQ_NUC_GB267_CO1_DADA2.fasta"  # Taxonomy database

# Core directories (modify if folder structure changes)
input_root <- here("eDNA", "inputs")
scripts_root <- here("eDNA", "scripts")
output_root <- here("eDNA", "outputs", project_id)
figures_root <- here("eDNA", "figures", project_id)
tax_root <- here("eDNA", "tax", tax_db)
subset_tax_root <- here("eDNA","tax", "midori_subset.fasta")

# Input paths
metadata_path <- here(input_root, "metadata", "metadata_unique.csv")
trimmed_path <- here(input_root, "trimmed", project_id, primer)

# Output paths
dada2_output <- here(output_root, paste0(project_id, "_dada2.rds"))
track_table_path <- here(output_root, paste0(project_id, "_read_changes.txt"))
nochim_output <- here(output_root, paste0(project_id, "_nochim_transposed.rds"))
nochim_csv <- here(output_root, paste0(project_id, "_nochim.csv"))
tax_table_path <- here(output_root, paste0(project_id, "_taxtable.rds"))
phyloseq_path <- here(output_root, paste0(project_id, "_phyloseq.rds"))
asv_seqs_path <- here(output_root, paste0(project_id, "_ASV_sequences.rds"))
clustered_phyloseq_path <- here(output_root, paste0(project_id, "_clustered_phyloseq.rds"))
vsearch_output_path <- file.path(output_root, "match_list97.txt")
curated_phyloseq_path <- here(output_root, paste0(project_id, "_lulu_curated.rds"))

# ----------------------
# CLEANING STEPS (must be before any data is loaded)
# ----------------------
# 1. Check and transpose OTU table if needed
if (file.exists(nochim_output)) {
  otu <- readRDS(nochim_output)
  # If OTU table has fewer rows than columns, likely needs transposing
  if (nrow(otu) < ncol(otu)) {
    message("Transposing OTU table to have OTUs as rows and samples as columns...")
    otu <- t(otu)
    saveRDS(otu, nochim_output)
  }
}

# 2. Check and fix metadata sample IDs for uniqueness
if (file.exists(metadata_path)) {
  meta <- read.csv(metadata_path, header=TRUE)
  if (any(duplicated(meta[[1]]))) {
    message("Making metadata sample IDs unique...")
    meta[[1]] <- make.unique(as.character(meta[[1]]), sep = "_")
    write.csv(meta, metadata_path, row.names=FALSE)
  }
}

# Now safe to load metadata
metadata <- read.csv(metadata_path, header=TRUE)

# Create directories if they don't exist
dir.create(output_root, showWarnings = FALSE)
dir.create(figures_root, showWarnings = FALSE)

################################################################
# (2) PIPELINE EXECUTION
################################################################

# (A) INITIAL QUALITY ASSESSMENT
source(file.path(scripts_root, "A_Quality_Assessment.R"))  # This script is a function to generate quality plots for sequence reads

# Execute quality plotting
quality_data <- generate_quality_plots(
  pathinput = trimmed_path,
  pathfigures = figures_root
)
# load and view quality plots
quality_data

# User-Defined Parameters (Modify After Viewing Plots)

# TRUNCATION PARAMETER GUIDELINES
# 1. truncLen: Set to positions where median quality > Q30
#    - Check 00_quality_forward.png and 00_quality_reverse.png
#    - Example: Forward drops at 220bp, Reverse at 200bp → c(220,200)
# 2. maxEE: Maximum expected errors (typically 2-3 for each read)
# 3. truncQ: Truncate when quality scores drop below this value (usually 2)

trunc_params <- list(
  truncLen = c(225, 215),  # Set based on quality plots
  maxEE = c(2, 2),         # Maximum expected errors
  truncQ = 2               # Truncate at first quality score < 2
)

# (B) INFER ASVs WITH DADA2
# This script filters + trims reads, infers Amplicon Sequence Variants (ASVs)

# for COI, remove non-specific amplification and filter by length 310–316 bp
COI_seqtab <- TRUE   # Set to TRUE if using COI primers, FALSE for 12S or others

# NovaSeq bins quality scores; we need to adapt the error model to account for this using the dada2 package makeBinnedQualErrfun(binnedQ)
NOVA <- TRUE # Set to TRUE if using NovaSeq data

source(file.path(scripts_root, "B_Infer_ASVs_DADA2.R"))   

# run DADA2 processing with ncores parellel processing 
processing_results <- run_dada2_processing(
  fnFs = quality_data$fnFs,
  fnRs = quality_data$fnRs,
  pathinput = trimmed_path,
  pathoutput = dada2_output,
  pathoutput_tracktable = track_table_path,
  pathoutput_nochim_rds = nochim_output,
  pathoutput_nochim_csv = nochim_csv,
  pathfigures = figures_root,
  trunc_params = trunc_params,
  ncores = 8
)
# saved track_table which shows read numbers through each step of the pipeline
# saved nochim.rds object which contains the final ASV table after chimera removal
# saved nochim.csv file which contains the final ASV table after chimera removal in csv format 

# (C) TAXONOMY ASSIGNMENT DADA2
# This is just a quick assignment to create a taxonomy table, and then a phyloseq object that can be used to detect contaminants

# To speed things up, we use a version of MIDORI with a few sequences
Midori_data <- read.FASTA(tax_root)
# Subset to the first 200 sequences
Midori_small <- Midori_data[1:200]
# Convert the DNAbin object to a list of character vectors
Midori_small_char <- as.character(Midori_small)
# NEW STEP: Convert all sequences to uppercase
Midori_small_char_upper <- lapply(Midori_small_char, toupper)
# Get the names of the sequences
sequence_names <- labels(Midori_small)
# Open a file connection for writing
file_conn <- file(subset_tax_root, "w")
# Loop through each sequence to write it manually
for (i in 1:length(Midori_small_char_upper)) {
  # Write the header line, starting with ">" and the sequence name
  writeLines(paste0(">", sequence_names[i]), file_conn)
  
  # Write the uppercase sequence
  writeLines(paste(Midori_small_char_upper[[i]], collapse = ""), file_conn)
}
# Close the file connection to save the file
close(file_conn)

source(file.path(scripts_root, "C_Assign_Taxonomy_DADA2.R"))   # This script assigns taxonomy using Dada2 Midori database to create an otu table

tax_results <- assign_taxonomy(
  seqtab_nochim_rds = nochim_output,
  tax_db_path = subset_tax_root,
  output_dir = output_root,
  project_id = project_id,
  ncores = 8,
  seed = 119
)
# saved taxtable.rds object which contains the taxonomy table for the ASVs
# saved tax.csv file which contains the taxonomy table for the ASVs in csv format
# saved otu.csv file (essentially a copy of nochim data)

# (D) Phyloseq Object Creation
# This script creates a phyloseq object
source(file.path(scripts_root, "D_Phyloseq.R"))

phy_results <- create_phyloseq(
  otu_table_rds = nochim_output,
  tax_table_rds = tax_table_path,
  metadata_path = metadata_path,
  output_dir = output_root,
  project_id = project_id,
  sample_id_col = 1,
  fix_mismatches = TRUE
)
# saved phyloseq.rds object which combines OTU table, taxonomy table, and metadata
# saved phy_samples.csv which contains sample data for verification

# Add the library size to the phyloseq object before continuing with decontamination
ps <- readRDS(phyloseq_path)
sample_data(ps)$lib_size <- sample_sums(ps)
saveRDS(ps, phyloseq_path)

# extract ASV sequences and write them to a FASTA file
# Load your ASV table (nochim_output)
asv_tab <- readRDS(nochim_output)
# Extract ASV sequences (row names)
asv_seqs <- rownames(asv_tab)
# Create a DNAStringSet object
asv_dna <- DNAStringSet(asv_seqs)
names(asv_dna) <- taxa_names(readRDS(phyloseq_path)) # Ensure names match phyloseq object
# Write to FASTA
writeXStringSet(asv_dna, filepath = file.path(output_root, paste0(project_id, "_ASV_sequences.fasta")))
# Save as RDS for easy loading later
saveRDS(asv_dna, file = asv_seqs_path)

################################################################
# (3) MARKER SPECIFIC STEPS
################################################################

## CO1 #########################################################

## Remove contaminants with Decontam ####
# COI requires additional steps to address nuclear mitochondrial pseudogenes

#reload phyloseq object and match ASV/OTU names
ps <- readRDS(phyloseq_path)
asv_seqs <- readRDS(asv_seqs_path)

source(file.path(scripts_root, "COI_A_Decontam.R"))

coi_decon <- run_COI_decontamination(
  physeq_path = phyloseq_path,
  asv_seqs_path = asv_seqs_path,
  output_dir = output_root,
  project_id = project_id,
  control_col = "sample_type",
  neg_controls = c("extraction control", "field control", "pcr control"),
  min_reads = 300,  # eDNA samples are expected to have relatively low reads
  prevalence_threshold = 0.05,
  max_n_ratio = 0.001,  # Stricter for COI 0.001
  frame_shift_check = TRUE,
  manual_remove_taxa = NULL # add sequences to remove manually here if needed
)

# cleaned phyloseq object after decontamination and COI-specific filtering
ps_clean <- coi_decon$phyloseq_clean

## Cluster ASVs with Decipher ####

#reload phyloseq object and match ASV/OTU names
ps <- readRDS(phyloseq_path)
asv_seqs <- readRDS(asv_seqs_path)

# Add reference sequences to phyloseq object
methods::slot(ps_clean, "refseq") <- asv_seqs

source(file.path(scripts_root, "COI_B_Cluster_ASVs.R")) # this script clusters similar ASVs into OTUs with 97% similarity

# Run clustering
ps_otu <- cluster_coi_asvs(
  ps_object = ps_clean,
  output_dir = output_root,
  project_id = project_id,
  cutoff = 0.03,       # 97% similarity threshold
  min_coverage = 0.8   # Minimum sequence overlap
)
# saved clustered_phyloseq.rds stored at clustered_phyloseq_path

## Create match list with Vsearch ####

# reload required rds files
ps_otu <- readRDS(clustered_phyloseq_path)
asv_seqs <- readRDS(asv_seqs_path)

# Match ASV names with phyloseq object
methods::slot(ps_otu, "refseq") <- asv_seqs

source(file.path(scripts_root, "COI_C_VSsearch_Prep.R")) # this script extracts ASVs and prepares them for Vsearch

# Run the OTU curation and vsearch prep step
curate_otus(
  ps_object = ps_otu,
  output_dir = output_root,
  project_id = project_id
)

## USE VSEARCH (In Terminal) ##

# set directory to where VSEARCH FASTA file was just saved 
# example: cd /path/to/folder/eDNA/testCOI/outputs
# then run vsearch with: 
# vsearch --usearch_global testCOI_otus.fasta --db testCOI_otus.fasta --self --id 0.84 --iddef 1 --userout match_list97.txt --userfields query+target+id --maxaccepts 0 --query_cov 0.9 --maxhits 10
#                          ^change this to your own .fasta file name

# After you have run the vsearch command in your terminal
# Read the vsearch output back into R
vsearch_matches <- read.table(vsearch_output_path, header = FALSE, col.names = c("OTUid", "hit", "match"))

# Clean up the data
vsearch_matches$OTUid <- as.character(vsearch_matches$OTUid)
vsearch_matches$hit <- as.character(vsearch_matches$hit)

# You can now use this 'vsearch_matches' data frame for further curation

## Curate OTUs with lulu ####

# reload ps_otu
ps_otu <- readRDS(clustered_phyloseq_path)

# Make sure the 'matchlist_name' variable is what you used in the previous step
vsearch_matches <- read.table(vsearch_output_path, header = FALSE, col.names = c("OTUid", "hit", "match"))
fasta_file_path <- file.path(output_root, paste0(project_id, "_otus.fasta"))
fasta_seqs <- Biostrings::readDNAStringSet(fasta_file_path)

source(file.path(scripts_root, "COI_D_Lulu_Curation.R"))

ps_curated <- curate_lulu(
  ps_object = ps_otu,
  matchlist_df = vsearch_matches
)

saveRDS(ps_curated, curated_phyloseq_path)


# saved curated phyloseq object to curated_phyloseq_path

## BLAST (in Terminal) ####
# To run BLAST, you need to have the BLAST+ command line tools installed
## Download latest MIDORI CO1 unique database from http://www.reference-midori.info/download.php# 
# unzip the downloaded file and save it to the tax_root directory


## Curate LCA with galaxy tool ####


## Create final phyloseq object ####

## 12S (MiFish) #########################################################
